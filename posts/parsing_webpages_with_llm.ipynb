{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From unstructured to structured: Parsing webpages with a Large Language Model (LLM)\n",
    "\n",
    "[In a recent article](https://hdembinski.github.io/posts/llama_index_rag.html), I showed how to set up a simple RAG system based on a locally run Large Language Model. I already praised the `ollama` library there, which makes it very easy to run LLMs locally. In this post, I will let the LLM parse a web page to extract data and return it in a structured format. More specifically, I will read a couple of web pages from InspireHEP about a few scientific papers on which I am a co-author and then convert these into a simple Markdown list with these references. That is an extract-transform-load (ETL) task in data engineering. Normally, one would write a rigid parser to solve this task, but with LLMs we can skip that and just describe the task in human language.\n",
    "\n",
    "And yes, there are easier ways to solve this particular task: InspireHEP allows one to download information about papers in machine readable format (BibTeX and others). The point is that this solution can be used also for other pages that do not offer access to their data in machine-readable format.\n",
    "\n",
    "I will use the `llama3-chatqa` model with 8b parameters, which is supposed to be good at this task. I got good results from this model after tweaking the prompt a lot. Larger models need less prompt engineering. A better but larger model for this task is `command-r` with 32b parameters, which I cannot run efficiently on a GPU with 8 GB RAM. \n",
    "\n",
    "I don't use `llama-index` in this post:\n",
    "\n",
    "- `llama-index` does not provide a locally-run reader for dynamic web pages that run JavaScript. The `ReadabilityWebReader` should work, but does not.\n",
    "- There are dependency conflicts between the `llama-index` and `ollama`.\n",
    "\n",
    "`llama-index` (like all the other libraries out there) offer lots of paid cloud-based services, but the support for locally running solutions is modest. More resources are allocated to support paid services, for obvious reasons.\n",
    "\n",
    "Fortunately, we don't need any functionality in `llama-index`, we can do it with a few lines of code from scratch. I use `playwright` to render the HTML and markdownify to convert to Markdown, and then I use `ollama` to generate my responses. You need to install these libraries to run the notebook\n",
    "\n",
    "- ollama\n",
    "- playwright\n",
    "- markdownify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading a dynamic web pages and convert HTML to Markdown\n",
    "\n",
    "The code for this part was written by ChatGPT. At least on Windows, the Playwright code cannot be run inside a Jupyter notebook, so I had to use a script. Here is the content of the strict, which downloads the dynamic HTML and converts it to Markdown and saves the Markdown files in the subdirectory `scraped`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "script = r\"\"\"\n",
    "from playwright.sync_api import sync_playwright\n",
    "from markdownify import markdownify as md\n",
    "from pathlib import Path\n",
    "\n",
    "urls = '''\n",
    "https://inspirehep.net/literature/1889335\n",
    "https://inspirehep.net/literature/2512593\n",
    "https://inspirehep.net/literature/2017107\n",
    "https://inspirehep.net/literature/2687746\n",
    "https://inspirehep.net/literature/2727838\n",
    "'''\n",
    "\n",
    "urls = [x.strip() for x in urls.split(\"\\n\") if x and not x.isspace()]\n",
    "\n",
    "\n",
    "def scrape_to_markdown(urls, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "\n",
    "        for url in urls:\n",
    "            output_fn = (\n",
    "                url.replace(\"://\", \"_\").replace(\"/\", \"_\").replace(\".\", \"_\") + \".md\"\n",
    "            )\n",
    "            ofile = output_dir / output_fn\n",
    "            page = browser.new_page()\n",
    "\n",
    "            page.goto(url)\n",
    "\n",
    "            # Wait for JavaScript-rendered content to load\n",
    "            page.wait_for_load_state(\"networkidle\")\n",
    "\n",
    "            rendered_html = page.content()\n",
    "\n",
    "            page.close()\n",
    "\n",
    "            markdown_content = md(rendered_html)\n",
    "\n",
    "            with open(ofile, \"w\", encoding=\"utf-8\") as file:\n",
    "                file.write(markdown_content)\n",
    "\n",
    "            print(f\"Saved {ofile!r}\")\n",
    "\n",
    "        browser.close()\n",
    "\n",
    "\n",
    "scrape_to_markdown(urls, \"scraped\")\n",
    "\"\"\"\n",
    "\n",
    "if not Path(\"scraped\").exists():\n",
    "    with open(\"scrape.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(script)\n",
    "\n",
    "    subprocess.run([\"python\", \"scrape.py\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of an example files looks like this:\n",
    "\n",
    "```\n",
    "When heavy ions meet cosmic rays: potential impact of QGP formation on the muon puzzle - INSPIREYou need to enable JavaScript to run this app.From 21 Dec 2023 to 06 Jan 2024 the INSPIRE team works on a reduced schedule and it will take a bit longer than usual to address your requests. Best wishes for the season![INSPIRE Logo](/)literature\n",
    "\n",
    "* Help\n",
    "* Submit\n",
    "* [Login](/user/login)\n",
    "\n",
    "[Literature](/literature)[Authors](/authors)[Jobs](/jobs)[Seminars](/seminars)[Conferences](/conferences) More... \n",
    "\n",
    "When heavy ions meet cosmic rays: potential impact of QGP formation on the muon puzzle\n",
    "--------------------------------------------------------------------------------------\n",
    "\n",
    "* [Tanguy Pierog](/authors/1025036)(\n",
    "  + [KIT, Karlsruhe, IAP](/institutions/1856897)),\n",
    "* [Sebastian Baur](/authors/1370189)(\n",
    "  + [Brussels U., IIHE](/institutions/902696)),\n",
    "* [Hans Dembinski](/authors/1035720)(\n",
    "  + [Tech. U., Dortmund (main)](/institutions/1088595)),\n",
    "* [Matías Perlin](/authors/1590214)(\n",
    "  + [KIT, Karlsruhe, IAP](/institutions/1856897) and\n",
    "  + [CNEA, Buenos Aires](/institutions/902700)),\n",
    "* [Ralf Ulrich](/authors/1035845)(\n",
    "  + [KIT, Karlsruhe, IAP](/institutions/1856897))\n",
    "\n",
    "Show All(6)20218 pagesPublished in: \n",
    "\n",
    "* *PoS* ICRC2021 (2021) 469\n",
    "Contribution to: \n",
    "\n",
    "* [+ ICRC 2021](/conferences/1776906), 469\n",
    "\n",
    "* Published: 2021\n",
    "DOI: \n",
    "\n",
    "* [10.22323/1.395.0469](//doi.org/10.22323/1.395.0469)\n",
    "View in: \n",
    "\n",
    "* [HAL Science Ouverte](https://hal.science/hal-03373282)\n",
    "[pdf](https://inspirehep.net/files/294f575f9507555012249728090abc50)citeclaim[reference search](/literature?q=citedby:recid:1928162)[9 citations](/literature?q=refersto:recid:1928162)\n",
    "### Citations per year\n",
    "\n",
    "202120222023202432Abstract: (SISSA)The deficit of muons in the simulation of extensive air showers is a long-standing problem and the origin of large uncertainties in the reconstruction of the mass of the high energy primary cosmic rays. Hadronic interaction models, re-tuned after early LHC data, have a more consistent description of the muon content among them but still disagree with data. Collective hadronization due to the formation of a quark gluon plasma (QGP) has already been studied as a possible cause for a larger production of muons under extreme conditions (rare, very central nuclear interactions), but without real success. However, in the view of the most recent LHC data, a collective hadronization phase might not only be limited to such extreme conditions. And because of its different ratio of electromagnetic to hadronic energy, a QGP may have the properties to solve the muon puzzle. This hypothesis is demonstrated using a theoretical approach and tested in a proper way by the modification of hadronic model spectra in CONEX to mimic the production of a QGP also in less extreme conditions with a possible large impact on air shower physics.\n",
    "\n",
    "* showers: atmosphere\n",
    "* cosmic radiation: primary\n",
    "* energy: hadronic\n",
    "* interaction: model\n",
    "* energy: high\n",
    "* muon: production\n",
    "* nucleus: interaction\n",
    "* model: hadronic\n",
    "* quark gluon: plasma\n",
    "* collective\n",
    "Show all (16)References(40)Figures(0)\n",
    "\n",
    "* [1]\n",
    "  #### [Air Shower Simulation with a New Generation of post-LHC Hadronic Interaction Models in CORSIKA](/literature/1687010)\n",
    "  \n",
    "  + [Tanguy Pierog](/authors/1025036)(\n",
    "    - [KIT, Karlsruhe](/institutions/911469))\n",
    "  + - *PoS* ICRC2017 (2018) 1100 •\n",
    "  + DOI: \n",
    "    - [10.22323/1.301.1100](//doi.org/10.22323/1.301.1100)edit\n",
    "\n",
    "[...]\n",
    "```\n",
    "\n",
    "The web page also contains all the references cited by the paper. I skipped that part here, which is not of interest for us. In fact, we can and should cut that part away in order to help the model focus on the relevant text piece and to not overload its context window.\n",
    "\n",
    "The converted Markdown contains mistakes, where the conversion process garbled up the structure of the document. Let's see whether the LLM can make sense of this raw text. We want it to extract the authors, the journal data, the title, and the DOI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data from raw text with a LLM\n",
    "\n",
    "We need to write a good prompt for our model, that explains what we want, ideally without ambiguity. Just like humans, LLMs are better at inferring a rule from concrete examples, than from an abstract description. In my prompt, I describe the general format and then illustrate it with concrete examples, which seems to work well.\n",
    "\n",
    "Also just like humans, LLMs pay more attention to context that is nearby. For transformer architectures, there is no fundamental reason for that, it just something the model learns during training. Therefore we put the instructions after the data, to make sure that our format is followed.\n",
    "\n",
    "[You can read more about prompt engineering elsewhere](https://github.com/brexhq/prompt-engineering?tab=readme-ov-file#chain-of-thought). I find it amusing that prompt engineering rediscovers good practices from education, which is no surprise: LLMs learned how to 'think' from texts written by humans, so they pick up our cognitive biases, too.\n",
    "\n",
    "Even so, the model sometimes generates garbage, because it is so small. To compensate, I run it three times for each document and pick the good results by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "from pathlib import Path\n",
    "\n",
    "input_dir = Path(\"scraped\")\n",
    "\n",
    "documents = [fn.open(encoding=\"utf-8\").read() for fn in input_dir.glob(\"*.md\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0: Roel Aaij et al., JHEP 01 (2022) 166, \"Measurement of prompt charged-particle production in pp collisions at s=13 TeV\", [10.1007/JHEP01(2022)166](https://doi.org/10.1007/JHEP01(2022)166)\n",
      "0.1: Roel Aaij et al., JHEP 01 (2022) 166, \"Measurement of prompt charged-particle production in pp collisions at s=13 TeV\", [10.1007/JHEP01(2022)166](https://doi.org/10.1007/JHEP01(2022)166)\n",
      "0.2: Roel Aaij et al., JHEP 01 (2022) 166, \"Measurement of prompt charged-particle production in pp collisions at s=13 TeV\", [10.1007/JHEP01(2022)166](https://doi.org/10.1007/JHEP01(2022)166)\n",
      "1.0: Johannes Albrecht, Hans Dembinski, Anatoli Fedynitch, Karl-Heinz Kampert, Astropart.Space Sci. 367 (2022) 3, 27, \"The Muon Puzzle in cosmic-ray induced air showers and its connection to the Large Hadron Collider\", [10.1007/s10509-022-04054-5](https://doi.org/10.1007/s10509-022-04054-5)\n",
      "1.1: Johanes Albrecht, Lorenzo Cazon, Hans Dembinski, Anatoli Fedynitch, Karl-Heinz Kampert, Astrophys.Space Sci., \"The Muon Puzzle in cosmic-ray induced air showers and its connection to the Large Hadron Collider\", [10.1007/s10509-022-04054-5](https://doi.org/10.1007/s10509-022-04054-5)\n",
      "1.2: Johannes Albrecht et al., Astrophys.Space Sci. 367 (2022) 3, \"The Muon Puzzle in cosmic-ray induced air showers and its connection to the Large Hadron Collider\", [10.1007/s10509-022-04054-5](https://doi.org/10.1007/s10509-022-04054-5)\n",
      "2.0: Hans Peter Dembinski, Ahmed Abdelmotteleb, Eur.Phys.J.C 82 (2022) 1043, \"A new maximum-likelihood method for template fits\", [10.1140/epjc/s10052-022-11019-z](https://doi.org/10.1140/epjc/s10052-022-11019-z).\n",
      "2.1: Hans Peter Dembinski, Ahmed Abdelmotteleb, Eur.Phys.J.C 82 (2022) 1043, \"A new maximum-likelihood method for template fits\", [10.1140/epjc/s10052-022-11019-z](https://doi.org/10.1140/epjc/s10052-022-11019-z).\n",
      "2.2: Hans Peter Dembinski, Ahmed Abdelmotteleb, Eur.Phys.J.C 82 (2022) 1043, \"A new maximum-likelihood method for template fits\", [10.1140/epjc/s10052-022-11019-z](https://doi.org/10.1140/epjc/s10052-022-11019-z).\n",
      "3.0: L. Cazon et al., PoS ICRC2023 (2023) 431, \"The muon measurements of Haverah Park and their connection to the muon puzzle\", [10.22323/1.444.0431](https://doi.org/10.22323/1.444.0431).\n",
      "3.1: L. Cazon et al., PoS ICRC2023 (2023) 431, \"The muon measurements of Haverah Park and their connection to the muon puzzle\", [10.22323/1.444.0431](https://doi.org/10.22323/1.444.0431).\n",
      "3.2: L. Cazon et al., PoS ICRC2023 (2023) 431, \"The muon measurements of Haverah Park and their connection to the muon puzzle\", [10.22323/1.444.0431](https://doi.org/10.22323/1.444.0431).\n",
      "4.0: Hans Dembinski, Michael Schmelling, \"Bias, variance, and confidence intervals for efficiency estimators in particle physics experiments\", [arXiv:2110.00294](https://arxiv.org/abs/2110.00294)\n",
      "4.1: Hans Dembinski, Michael Schmelling, arXiv:2110.00294, \"Bias, variance, and confidence intervals for efficiency estimators in particle physics experiments\", [10.1007/JHEP01(2022)166](https://doi.org/10.1007/JHEP01(2022)166)\n",
      "4.2: Hans Dembinski, Michael Schmelling, \"Bias, variance, and confidence intervals for efficiency estimators in particle physics experiments\", arXiv:2110.00294\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "Extract the authors, title, journal info, and DOI from the text in <input> tags.\n",
    "\n",
    "<input>\n",
    "{text}\n",
    "</input>\n",
    "\n",
    "Return the result in Markdown format in this format:\n",
    "\n",
    "First and last name of first author, First and last name of second author, ..., journal reference, \"The title\", [DOI](DOI URL)\n",
    "\n",
    "Requirements:\n",
    "- If there are more than four authors, use `First author et al.` instead of listing all authors.\n",
    "- The journal reference must not contain *italic* or **bold** emphasis.\n",
    "- The list of authors must be author names only separated by commas.\n",
    "- Convert LaTeX formulas into equivalent plain text.\n",
    "\n",
    "Examples that pass the check:\n",
    "- Roel Aaij et al., JHEP 01 (2022) 166, \"Measurement of prompt charged-particle production in pp collisions at s=13 TeV\", [10.1007/JHEP01(2022)166](https://doi.org/10.1007/JHEP01(2022)166)\n",
    "- Flavia Gesualdi et al., PoS ICRC2021 (2021) 473, \"On the muon scale of air showers and its application to the AGASA data\", [10.22323/1.395.0473](https://doi.org/10.22323/1.395.0473)\n",
    "\n",
    "The extracted reference:\n",
    "\"\"\"\n",
    "\n",
    "for idoc, doc in enumerate(documents):\n",
    "    # strip the bibliography block\n",
    "    d = doc[:doc.index(\"###\")]\n",
    "    prompt = prompt_template.format(text=d)\n",
    "    for trial in range(3):\n",
    "        # a low temperate seems to make the output more reliable\n",
    "        response = ollama.generate(model='llama3-chatqa', prompt=prompt, options={\"temperature\": 0.3, \"seed\": trial})\n",
    "        # tiny bit of post-processing: replace newlines with spaces, trim whitespace\n",
    "        text = response.response.replace('\\n', '').strip()\n",
    "        print(f\"{idoc}.{trial}: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The output is pretty good for such a small model, also considering the modest quality of the input. The LLM correctly identified the relevant pieces: authors, journal information, and the paper title. Which is which is not really obvious from the raw input. A simple pattern matching wouldn't be able to do it. The model uses its knowledge how a name looks like, how a journal reference looks like and so on.\n",
    "\n",
    "One should note, however, that the output quality varies considerably from run to run.\n",
    "\n",
    "### Mistake analysis\n",
    "\n",
    "- The model ignored the instruction to abbreviate long author lists, but larger models are able to do that. While the model is able to count authors in isolation, it fails to comprehend rules such as \"shorten the author list if it contains more than X names\". It will even claim nonsense like that \"three names are more than four\".\n",
    "- The model failed to convert the LaTeX in the title of the first paper correctly, `$\\sqrt{s} = ...$` just became `s = ...`, while it should be `sqrt(s) = ...`.\n",
    "- The model rarely makes spelling mistakes. It rather makes higher-level mistakes, like omitting an author than misspelling them.\n",
    "- The last reference is special, this is an unpublished preprint. It does not have a DOI, for example. The model nevertheless produced a reasonable entry sometimes, but in other cases it hallucinates DOIs that do not exist.\n",
    "\n",
    "### Failed attempts\n",
    "\n",
    "One technique to improve the output of a flawed model is to let the model critique its previous output and suggest improvements. I tried this, but this small model is unable to critique itself, it always accepts its own answer even if it does not adhere to the requested format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
