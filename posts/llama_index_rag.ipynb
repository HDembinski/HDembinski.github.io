{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Large Language Models (LLMs) locally for Retrieval-Augmented-Generation (RAG) Systems with full privacy\n",
    "\n",
    "**tl;dr:** You can run small LLMs locally on your consumer PC and with ollama that's very easy to set up. It is fun to chat with an LLM locally, but it gets really interesting when you build RAG systems or agents with your local LLM. I show you an example of a RAG-System built with llama-index.\n",
    "\n",
    "## Running small LLMs locally with quantization\n",
    "\n",
    "Large language models are large, mindboggingly large. Even if we had the source code and the weights of ChatGPTs GPT-4o model, with its (probably, the exact size is not known) 1,800b parameters - that is b for billion - it would be about 3 TB in size if every paramter is stored as a 16 bit float. Difficult to fit into your RAM!\n",
    "\n",
    "`<rant>`\n",
    "We could use proper SI notation, '1800G' or '1.8T' instead of '1800b' ðŸ˜ž, since 'billion' means different things in different languages, but here we are.\n",
    "`</rant>`\n",
    "\n",
    "But nevermind, we don't have the code and weights anyway. So what about open source models? While the flagships are still too large, there is a vibrant community on the HuggingFace platform that makes and improves models that have only **8b** to **30b** parameters, and those models are not useless. Meta has recently released a language model llama-3.2 with only **3b** parameters. While you cannot expect the same detailed knowledge about the world and attention span as the flagship models, these models still produce coherent text and you can have decent short conversations with them. I would recommend to use at least an **8b** model, because the smaller models likely won't follow your prompt very well.\n",
    "\n",
    "An 8b model is 200 times smaller than GPT-4o, but still has a size of about 15 GB. It fits into your CPU RAM, but you want it to fit onto your GPU. If it does not fit completely onto the GPU, a part of the calculation has to be done with the CPU, and that will slow down the generation dramatically. Memory transfer speed is the bottleneck.\n",
    "\n",
    "Fortunately, one can quantize the parameters quite strongly without loosing much. It turns out one can go down to 4 or 5 bits per parameter without loosing much (about one percent in benchmarks compared to the original model). This finally brings these models down to a size that fits onto consumer GPUs. You need some extra memory for the code and context window as well.\n",
    "\n",
    "**If you are interested in this sort of thing and plan to buy a GPU soon, take one with at least 16 GB of RAM. GPU speed does not really matter.**\n",
    "\n",
    "There are a couple of libraries which allow you to run these quantized models, but the best one by far is **Ollama** in my experience. Ollama is really easy to install and use. It successfully hides a lot of the complexity from you, and gives you easy start into the world of runnig local LLMs.\n",
    "\n",
    "I had a lot of fun trying out different models. There are leaderboards ([Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and [Chatbot Arena](https://lmarena.ai/)) which help to select good candidates. I noticed large differences in perceived quality among models with the same size. Generally, I recommend finetuned versions of the llama-3.1:8b and gemma2:9b models by the community. If you want to skip over that, then try out mannix/gemma2-9b-simpo, and if you have at least 16GB of GPU RAM, gemma2:27b.\n",
    "\n",
    "## Great, I have a local LLM running, now what?\n",
    "\n",
    "Having an LLM running locally is nice and all, but for programming and asking questions about the world, the free tiers of ChatGPT and Claude are better. The real interesting use case for local LLMs is to chat with your documents using retrieval augmented generation (RAG).\n",
    "\n",
    "There is great synergy in running a RAG System with a local LLM.\n",
    "- You can keep your local documents private. Nothing will ever be transferred to the cloud.\n",
    "- No additional costs. If you want to use the API of ChatGPT or Claude, you have to pay eventually. That's especially annoying while you are still developing, when you will run the LLMs over and over to test your application.\n",
    "- Local LLMs lack detailed world knowledge, but the RAG-System complements that lack of knowledge. Without RAG, local LLMs hallucinate a lot, but with RAG they will provide factual knowledge.\n",
    "\n",
    "A general advantage of RAG is that you can look into the text pieces that the LLM used to formulate its answer, which turns the LLM from a black box into a (nearly) white box.\n",
    "\n",
    "## Building a simple RAG System with llama-index\n",
    "\n",
    "For a RAG system, you need to convert your documents into plain text or Markdown, and an index to pull up relevant pieces from this corpus according to your query. There is currently gold-rush around developing converters for all kinds of documents into LLM-readable text, especially when it comes to PDFs. People try to make you to pay for this service. For PDFs, a free alternative that runs locally is **pymupdf4llm**. If your documents contain images, you can also run a multi-model LLM like llama-3.2-vision to make text descriptions for these images automatically.\n",
    "\n",
    "Once you have your documents in plain text, you can split into mouth-sized pieces (mouth-sized for your LLM and its (small) context window) and use an embedding model to compute semantic vectors for each piece. These vectors magically encode semantic meaning of text, and can be used to find pieces that are relevant to a query using cosine similiarity - that's essentially a dot-product of the vectors. It is hard to believe that this works, but it actually does. Search via embeddings is superior to keyword search, but I can also say from experience that it is not a silver bullet. The best RAG Systems combine keywords with embeddings in some way. Using a good embedding model is key. If you use a model trained for english on German text, for example, it won't perform well, or if your documents contain lots of technical language that the embedding model was not trained on.\n",
    "\n",
    "Thankfully, Ollama also offers embedding models, so you can run these locally as well. I found that mxbai-embed-large works well for both english and German text.\n",
    "\n",
    "Writing a RAG from scratch with Ollama is not too hard, but it usually pays off to use a well-designed library to do the grunt work, and then start to improve from there. I compared many libraries, and can confidently recommend **llama-index** as the best one by far. It is feature-rich and well designed: little boilerplate code for simple things, yet easy to extend. The workflow system especially is really well designed. Just their (good) documentation is annoyingly difficult to find, they try to push you to their paid cloud services (did I mention, there is a gold rush...).\n",
    "\n",
    "Below, I show you a RAG demo system, where I pull in Wikipedia pages about the seven antique world wonders, I then ask some questions about the Rhodes statue and the Hanging Gardens. As I am German, I wanted to see how well this works with German queries on German documents. That is not trivial, because both the LLM and the embedding model then have to understand German. I compare the result with and with RAG. Without RAG, the model will hallucinate details. With RAG, it follows the facts in the source documents closely. It is really impressive.\n",
    "\n",
    "To run this, you need to install a couple of Python packages:\n",
    "\n",
    "- ollama\n",
    "- llama-index\n",
    "- llama-index-llms-ollama\n",
    "- llama-index-embeddings-ollama\n",
    "- llama-index-readers-wikipedia\n",
    "- wikipedia\n",
    "- mistune\n",
    "- ipython\n",
    "\n",
    "Mistune renders Markdown to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import textwrap\n",
    "import mistune\n",
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "def wrap(s):\n",
    "    return \"\\n\".join(textwrap.wrap(s, replace_whitespace=False))\n",
    "\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large\")\n",
    "\n",
    "Settings.llm = Ollama(model=\"mannix/gemma2-9b-simpo\", request_timeout=1000)\n",
    "\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=512, chunk_overlap=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the German Wikipedia\n",
    "documents = WikipediaReader().load_data(\n",
    "    pages=[\n",
    "        \"Zeus-Statue des Phidias\",\n",
    "        \"Tempel der Artemis in Ephesos\",\n",
    "        \"Pyramiden von Gizeh\",\n",
    "        \"Pharos von Alexandria\",\n",
    "        \"Mausoleum von Halikarnassos\",\n",
    "        \"Koloss von Rhodos\",\n",
    "        \"HÃ¤ngende GÃ¤rten der Semiramis\",\n",
    "    ], lang_prefix=\"de\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HansDembinski\\source\\blog\\.pixi\\envs\\default\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 143.06it/s]\n",
      "Generating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 76/76 [00:07<00:00,  9.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lots of stuff is happening here. This splits the seven pages into chunks of texts, \n",
    "# and computes an embedding vector for each chunk, in the end we have 76 chunks of text\n",
    "# that the LLM can use. We don't need to pass the embedding model or the text splitter\n",
    "# explicitly, they are pulled from the Settings object.\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some questions for the LLM about facts regarding two of the seven wonders\n",
    "question = (\n",
    "    \"Aus welchen Materialien wurde der Koloss von Rhodos konstruiert?\",\n",
    "    \"Beschreibe die Pose des Koloss von Rhodos.\",\n",
    "    \"War der Koloss von Rhodos als nackte oder bekleidete Figur dargestellt?\",\n",
    "    \"In welcher Stadt befanden sich die HÃ¤ngenden GÃ¤rten?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Aus welchen Materialien wurde der Koloss von Rhodos konstruiert?</h1>\n",
       "<h2>Ohne RAG</h2>\n",
       "<p>Der Koloss von Rhodos, eines der sieben Weltwunder der Antike, wurde\n",
       "hauptsÃ¤chlich aus <strong>Stein und Bronze</strong> errichtet.  Die genaue\n",
       "Zusammensetzung und Bauweise sind jedoch aufgrund fehlender erhaltener\n",
       "BauplÃ¤ne und archÃ¤ologischer Funde nicht vollstÃ¤ndig rekonstruierbar.\n",
       "Historiker und ArchÃ¤ologen gehen jedoch basierend auf damaligen\n",
       "Bautechniken und schriftlichen Quellen wie folgt vor:</p>\n",
       "<ul>\n",
       "<li><strong>Fundament\n",
       "und Sockel:</strong>  Das gewaltige Fundament des Kolosses wurde aus\n",
       "<strong>massivem regionalem Kalkstein</strong> gebaut, der fÃ¼r seine\n",
       "WiderstandsfÃ¤higkeit bekannt war. Dieser Stein lieferte die stabile\n",
       "Basis auf der die Bronzestatur stand.</li>\n",
       "<li><strong>Skulptur selbst:</strong> Der\n",
       "eigentliche Koloss bestand aus <strong>Bronze</strong>, einem Material, das in der\n",
       "damaligen Zeit fÃ¼r groÃŸe Skulpturen bevorzugt wurde. Die Bronze wurde\n",
       "wahrscheinlich durch <strong>Gussverfahren</strong> in mehreren Teilen hergestellt,\n",
       "die anschlieÃŸend zusammengefÃ¼gt wurden. Vermutlich wurden komplexe\n",
       "<strong>Riegel und Gelenke</strong> aus Bronze verwendet, um die beweglichen\n",
       "KÃ¶rperteile des Kolosses zu ermÃ¶glichen und StabilitÃ¤t zu\n",
       "gewÃ¤hrleisten.</li>\n",
       "<li><strong>Innenausbau:</strong>  Es wird vermutet, dass der innere\n",
       "Aufbau des Kolosses aus <strong>Holz und weiteren Baumaterialien</strong> bestand,\n",
       "um Gewicht zu sparen und gleichzeitig eine stabile Struktur zu\n",
       "schaffen.  MÃ¶glicherweise enthielt er auch  <strong>VerstÃ¤rkungen aus\n",
       "Eisen</strong> an entscheidenden Stellen.</li>\n",
       "<li><strong>Dekoration:</strong>  Details wie\n",
       "GesichtszÃ¼ge, Kleidung und Waffen des Kolosses wurden\n",
       "hÃ¶chstwahrscheinlich aus <strong>poliert und vergoldeter Bronze</strong>\n",
       "geschaffen, um einen beeindruckenden Glanz zu erzeugen.\n",
       "<strong>Zusammenfassend lÃ¤sst sich sagen:</strong> Der Koloss von Rhodos war eine\n",
       "Meisterleistung aus Stein und Bronze, die durch innovative\n",
       "Bautechniken und kunstvolle Verarbeitungstechniken zum Leben erweckt\n",
       "wurde.  Obwohl uns genaue Details fehlen, zeichnet sich das Bild eines\n",
       "monumentalen Werkes aus, das die technischen und kÃ¼nstlerischen\n",
       "FÃ¤higkeiten der Antike exemplarisch demonstriert.</li>\n",
       "</ul>\n",
       "<h2>RAG</h2>\n",
       "<p>Die kolossale Statue des Helios bestand aus <strong>Bronze</strong>.</p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Beschreibe die Pose des Koloss von Rhodos.</h1>\n",
       "<h2>Ohne RAG</h2>\n",
       "<p>Der Koloss von Rhodos, eines der Sieben Weltwunder der Antike, war\n",
       "nicht in einer typischen, statischen Pose dargestellt, sondern in\n",
       "einer <strong>dynamischen und majestÃ¤tischen Haltung</strong>, die seine GrÃ¶ÃŸe und\n",
       "Macht unterstrich.</p>\n",
       "<p><strong>Detaillierte Beschreibung:</strong></p>\n",
       "<ul>\n",
       "<li><strong>Stehende\n",
       "Figur:</strong> Der Koloss stand <strong>aufrecht und blickte Ã¼ber die Hafenstadt\n",
       "Rhodos</strong>. Er war somit keine sitzende oder kniende Figur, sondern eine\n",
       "beeindruckende, erhebende PrÃ¤senz.</li>\n",
       "<li><strong>Gesten und Haltung:</strong> WÃ¤hrend\n",
       "genaue Darstellungen der HÃ¤nde fehlen, wird angenommen, dass er mit\n",
       "<strong>ausgestreckten Armen</strong> stand, die mÃ¶glicherweise <strong>leicht nach vorne\n",
       "geneigt</strong> waren, um eine offene und gastfreundliche Geste zu\n",
       "vermitteln, passend zu Rhodos als bedeutender Handelshafen. Sein\n",
       "<strong>KÃ¶rper zeigte eine natÃ¼rliche, leicht nach vorne geneigte Haltung</strong>,\n",
       "welche Dynamik und Bewegung suggerierte, anstatt starres Stehen.</li>\n",
       "<li><strong>Gesichtsausdruck:</strong>  Die Darstellung des Gesichts war wahrscheinlich\n",
       "<strong>seren und majestÃ¤tisch</strong>, vielleicht mit einem Hauch von stolzem\n",
       "Ãœberlegenheit, passend zu seiner Funktion als Symbol der Stadt und\n",
       "ihrer StÃ¤rke. Es fehlte vermutlich an einem aggressiven oder\n",
       "kÃ¤mpferischen Ausdruck.</li>\n",
       "<li><strong>Gewandtheit:</strong> Obwohl aus Bronze\n",
       "gefertigt, sollte der Koloss trotz seiner GrÃ¶ÃŸe <strong>elegant und\n",
       "geschmeidig wirken</strong>, nicht schwerfÃ¤llig oder statisch. Die\n",
       "Proportionen und die Modellierung des KÃ¶rpers sollten  Bewegung und\n",
       "Leichtigkeit suggerieren, Ã¤hnlich einem modernen Skulpturenkonzept der\n",
       "&quot;dynamischen Haltung&quot;.</li>\n",
       "<li><strong>Blickrichtung:</strong> Der Blick des Kolosses war\n",
       "vermutlich <strong>Ã¼ber den Hafen gerichtet</strong>, symbolisch die Stadt und ihre\n",
       "Schifffahrt beschÃ¼tzend und gleichzeitig die Ankunft von HandelsgÃ¼tern\n",
       "und Reisenden willkommen heiÃŸen.</li>\n",
       "</ul>\n",
       "<p>Zusammenfassend lÃ¤sst sich sagen,\n",
       "dass die Pose des Kolosses von Rhodos eine <strong>bewusste Kombination aus\n",
       "Kraft, Offenheit und majestÃ¤tischer WÃ¼rde</strong> darstellte, welche die\n",
       "Werte und den Status der Stadt Rhodos perfekt widerspiegelte.</p>\n",
       "<h2>RAG</h2>\n",
       "<p>Obwohl keine direkte, antike Darstellung existiert, wird vermutet, der\n",
       "Koloss stand in einer majestÃ¤tischen, aufrechten Haltung als nackter\n",
       "junger Mann mit langem, lockigem Haar und einem Strahlenkranz. Details\n",
       "Ã¼ber eine besonders dynamische oder ausgeprÃ¤gte Pose sind jedoch\n",
       "unklar.</p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>War der Koloss von Rhodos als nackte oder bekleidete Figur dargestellt?</h1>\n",
       "<h2>Ohne RAG</h2>\n",
       "<p>Der Koloss von Rhodos wurde <strong>traditionell als bekleidete Figur</strong>\n",
       "dargestellt. Obwohl genaue Darstellungen und Details aufgrund des\n",
       "Untergangs der Statue verloren gingen, liefern uns antike Quellen und\n",
       "Interpretationen Hinweise auf ihre Kleidung:</p>\n",
       "<ul>\n",
       "<li><strong>Heiligtumsfunktion:</strong> Der Koloss stand als Tribut an den Gott Helios\n",
       "im Heiligtum auf der Insel Rhodos.  GÃ¶tterdarstellungen in dieser Zeit\n",
       "waren oft mit Kleidung geschmÃ¼ckt, die ihre Macht, Stellung und den\n",
       "Kontext ihrer Verehrung symbolisierten. Es ist daher logisch\n",
       "anzunehmen, dass auch der Koloss entsprechend bekleidet war.</li>\n",
       "<li><strong>Antike Beschreibungen:</strong>  Plinius der Ã„ltere, ein rÃ¶mischer\n",
       "Geschichtsschreiber und Naturforscher, beschreibt den Koloss als eine\n",
       "&quot;herrliche Gestalt&quot; mit &quot;kleidernder Pracht&quot;.  Obwohl nicht\n",
       "spezifisch, deutet dies auf Kleidung hin.</li>\n",
       "<li><strong>Stilistische\n",
       "Parallelen:</strong> Vergleichbare gigantische Figuren aus der griechischen\n",
       "Kunst, wie z.B. der Zeus von Olympia, waren oft bekleidet, um ihre\n",
       "majestÃ¤tische Erscheinung zu unterstreichen. Der Koloss wÃ¼rde in\n",
       "diesem Kontext wahrscheinlich einem Ã¤hnlichen Stil folgen.</li>\n",
       "</ul>\n",
       "<p>Es gibt\n",
       "<strong>keine zuverlÃ¤ssigen Quellen</strong>, die einen nackten Koloss suggerieren.\n",
       "Die Annahme einer bekleideten Figur basierend auf historischem Kontext\n",
       "und kÃ¼nstlerischen Praktiken der Zeit erscheint daher am\n",
       "wahrscheinlichsten.</p>\n",
       "<h2>RAG</h2>\n",
       "<p>Basierend auf Annahmen und VergleichsmÃ¶glichkeiten mit Kunstwerken der\n",
       "damaligen Zeit, wird vermutet, dass die Figur wahrscheinlich als\n",
       "<strong>nackter</strong> junger Mann dargestellt wurde.</p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>In welcher Stadt befanden sich die HÃ¤ngenden GÃ¤rten?</h1>\n",
       "<h2>Ohne RAG</h2>\n",
       "<p>Die legendÃ¤ren HÃ¤ngenden GÃ¤rten wurden im antiken <strong>Babylon</strong>\n",
       "vermutet, der Hauptstadt des <strong>babylonischen Reiches</strong> in Mesopotamien\n",
       "(heute im Irak).</p>\n",
       "<p>Obwohl ihre Existenz historisch nicht zweifelsfrei\n",
       "belegt ist und ihre genaue Lage weiterhin diskutiert wird, stÃ¼tzt sich\n",
       "die traditionelle Zuschreibung auf:</p>\n",
       "<ul>\n",
       "<li><strong>Griechische Quellen:</strong>\n",
       "Geschichtsschreiber wie Diodor von Sizilien und Strabo aus dem 1.\n",
       "Jahrhundert v. Chr. beschrieben prÃ¤chtige Gartenanlagen in Babylon,\n",
       "die von kÃ¼nstlichen Terrassen mit BewÃ¤sserungssystemen hoch in die\n",
       "Luft ragten. Diese Beschreibungen passen zum Bild der &quot;HÃ¤ngenden\n",
       "GÃ¤rten&quot;.</li>\n",
       "<li><strong>ArchÃ¤ologische Hinweise:</strong>  WÃ¤hrend definitive Beweise\n",
       "fÃ¼r die GÃ¤rten selbst noch fehlen, fanden ArchÃ¤ologen in Babylon\n",
       "Ãœberreste komplexer BewÃ¤sserungssysteme und kÃ¼nstlicher Terrassen, die\n",
       "die technische Grundlage fÃ¼r solche monumentalen Anlagen\n",
       "unterstÃ¼tzten.</li>\n",
       "<li><strong>Babylonische Kultur:</strong> Das babylonische Reich war\n",
       "bekannt fÃ¼r seine beeindruckende Architektur, Ingenieurskunst und\n",
       "Liebe zur Gartenkultur. Es erscheint somit plausibel, dass eine solche\n",
       "prachtvolle Konstruktion in dieser Stadt entstanden sein kÃ¶nnte.\n",
       "Obwohl die genaue geographische Position innerhalb Bablons noch nicht\n",
       "vollstÃ¤ndig geklÃ¤rt ist, wird die <strong>NÃ¤he zum Euphratfluss</strong> und die\n",
       "<strong>stÃ¤dtische Umgebung</strong> als wahrscheinlichster Standort angesehen, der\n",
       "den BedÃ¼rfnissen der komplexen BewÃ¤sserungssysteme und der damaligen\n",
       "Stadtstruktur entsprach.</li>\n",
       "</ul>\n",
       "<h2>RAG</h2>\n",
       "<p>Die genaue Stadt, in der sich die HÃ¤ngenden GÃ¤rten befanden, ist in\n",
       "den historischen Aufzeichnungen <strong>nicht eindeutig identifiziert</strong>.</p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A lot of stuff is happening here behind the scenes: a query engine is constructed\n",
    "# from the document index. The query engine computes an embedding for the query, and\n",
    "# selects 10 text pieces that are most similar to the query. It then prompts the \n",
    "# LLM with our question and provides the text pieces as context.\n",
    "engine = index.as_query_engine(similarity_top_k=10)\n",
    "\n",
    "show_sources = False\n",
    "\n",
    "# Now we ask our questions. Set show_sources=True to see which text pieces were used.\n",
    "# For reference, we compare the RAG answer (\"RAG\") with a plain LLM query (\"Ohne RAG\"). \n",
    "for q in question:\n",
    "    q2 = q + \" Antworte detailliert auf Deutsch.\"\n",
    "    \n",
    "    response = Settings.llm.complete(q2)\n",
    "    rag = engine.query(q2)\n",
    "\n",
    "    s = f\"# {q}\\n\\n## Ohne RAG\\n\\n{wrap(response.text)}\\n\\n## RAG\\n\\n{wrap(rag.response)}\"\n",
    "\n",
    "    if show_sources:\n",
    "        s += \"\\n\\n## Sources\\n\\n\"\n",
    "        for node in rag.source_nodes:\n",
    "            s += f\"### Score {node.score}\\n{wrap(node.text)}\\n\\n\"\n",
    "\n",
    "    s = mistune.html(s)\n",
    "    display_html(s, raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers without RAG are much nicer to read, but contain halluciations, while the RAG answers are dull, brief, but factually correct. The behavior of the LLM without RAG is a consequence of human preference optimization. The LLM generates answers by default that *look nice* to humans.\n",
    "\n",
    "The RAG answer is very short, because the internal prompt of llama-index asks the LLM to only use information provided by the RAG system and not use its internal knowledge. It is therefore not a bug but a feature that the answer of the LLM is so short: it faithfully tries to only make statements that are covered by the text pieces. \n",
    "\n",
    "1. Question is about the materials used to construct the Rhodes statue.\n",
    "\n",
    "The standard LLM claims that wood was used in the construction of the Rhodes statue, but there are no records in the Wikipedia about that. The RAG answer is factual correct, it mentions bronce. The Wikipedia article also mentions other materials, but either the LLM missed those.\n",
    "\n",
    "2. Question is about the pose of the Rhodes statue.\n",
    "\n",
    "The standard LLM gives a lot of hallucinated detail. We don't know much about the pose, and the short RAG answer summarises that.\n",
    "\n",
    "3. Question is about whether the statue was clothed or naked.\n",
    "\n",
    "The standard LLM says it was clothed, probably because a lot of the antique statues were clothed, but the RAG answer is correct, the statue was naked as far as we know.\n",
    "\n",
    "4. Question is about the city in which the Hanging Gardens were supposed to be located.\n",
    "\n",
    "The standard LLM gives the correct answer in this case, Babylon. The RAG answer speaks about the location relative to the palace, but does not mention the city. This is the only case where the RAG answer is worse, although not factually incorrect. The failure in this case is related to the index, which fails to retrieve the right text piece with the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "RAG works very well even with small local LLMs. The caveats of small LLMs (lack of world knowledge) are compensated by RAG. The RAG answers are very faithful to the source in our example and contain no hallucinations. The use of local LLMs allows us to avoid additional costs and keeps our data private.\n",
    "\n",
    "The main challenge in setting up a RAG is the index. Finding all relevant pieces of information, without adding too many irrelevant pieces, is a hard problem. There are multiple ways to refine the basic RAG formula:\n",
    "\n",
    "- Getting more relevant pieces by augmenting the source documents with metadata like tags or LLM-generated summaries for larger sections.\n",
    "- Smarter text segmentation based on semantic similarity or logical document structure.\n",
    "- Postprocessing the retrieved documents, by letting a LLM rerank them according to their relevance for the query.\n",
    "- Asking the LLM to critique its answer, and then to refine it based on the critique.\n",
    "- ... and many other ways, it is an open and active field.\n",
    "\n",
    "Have a look into the [llama-index documentation](https://docs.llamaindex.ai/en/stable/examples/) for more advanced RAG workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
