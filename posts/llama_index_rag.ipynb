{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Large Language Models (LLMs) locally for Retrieval-Augmented-Generation (RAG) Systems with full privacy\n",
    "\n",
    "**tl;dr:** You can run small LLMs locally on your consumer PC and with ollama that's very easy to set up. It is fun to chat with an LLM locally, but it gets really interesting when you build RAG systems or agents with your local LLM. I show you an example of a RAG-System built with llama-index.\n",
    "\n",
    "## Running small LLMs locally with quantization\n",
    "\n",
    "Large language models are large, mindboggingly large. Even if we had the source code and the weights of ChatGPTs GPT-4o model, with its (probably, the exact size is not known) 1,800b parameters - that is b for billion - it would be about 3 TB in size if every paramter is stored as a 16 bit float. Difficult to fit into your RAM!\n",
    "\n",
    "`<rant>`\n",
    "We could use proper SI notation, '1800G' or '1.8T' instead of '1800b' üòû, since 'billion' means different things in different languages, but here we are.\n",
    "`</rant>`\n",
    "\n",
    "But nevermind, we don't have the code and weights anyway. So what about open source models? While the flagships are still too large, there is a vibrant community on the HuggingFace platform that makes and improves models that have only **8b** to **30b** parameters, and those models are not useless. Meta has recently released a language model llama-3.2 with only **3b** parameters. While you cannot expect the same detailed knowledge about the world and attention span as the flagship models, these models still produce coherent text and you can have decent short conversations with them. I would recommend to use at least an **8b** model, because the smaller models likely won't follow your prompt very well.\n",
    "\n",
    "An 8b model is 200 times smaller than GPT-4o, but still has a size of about 15 GB. It fits into your CPU RAM, but you want it to fit onto your GPU. If it does not fit completely onto the GPU, a part of the calculation has to be done with the CPU, and that will slow down the generation dramatically. Memory transfer speed is the bottleneck.\n",
    "\n",
    "Fortunately, one can quantize the parameters quite strongly without loosing much. It turns out one can go down to 4 or 5 bits per parameter without loosing much (about one percent in benchmarks compared to the original model). This finally brings these models down to a size that fits onto consumer GPUs. You need some extra memory for the code and context window as well.\n",
    "\n",
    "**If you are interested in this sort of thing and plan to buy a GPU soon, take one with at least 16 GB of RAM. GPU speed does not really matter.**\n",
    "\n",
    "There are a couple of libraries which allow you to run these quantized models, but the best one by far is **Ollama** in my experience. Ollama is really easy to install and use. It successfully hides a lot of the complexity from you, and gives you easy start into the world of runnig local LLMs.\n",
    "\n",
    "I had a lot of fun trying out different models. There are leaderboards ([Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) and [Chatbot Arena](https://lmarena.ai/)) which help to select good candidates. I noticed large differences in perceived quality among models with the same size. Generally, I recommend finetuned versions of the llama-3.1:8b and gemma2:9b models by the community. If you want to skip over that, then try out mannix/gemma2-9b-simpo, and if you have at least 16GB of GPU RAM, gemma2:27b.\n",
    "\n",
    "## Great, I have a local LLM running, now what?\n",
    "\n",
    "Having an LLM running locally is nice and all, but for programming and asking questions about the world, the free tiers of ChatGPT and Claude are better. The real interesting use case for local LLMs is to chat with your documents using retrieval augmented generation (RAG).\n",
    "\n",
    "There is great synergy in running a RAG System with a local LLM.\n",
    "- You can keep your local documents private. Nothing will ever be transferred to the cloud.\n",
    "- No additional costs. If you want to use the API of ChatGPT or Claude, you have to pay eventually. That's especially annoying while you are still developing, when you will run the LLMs over and over to test your application.\n",
    "- Local LLMs lack detailed world knowledge, but the RAG-System complements that lack of knowledge. Without RAG, local LLMs hallucinate a lot, but with RAG they will provide factual knowledge.\n",
    "\n",
    "A general advantage of RAG is that you can look into the text pieces that the LLM used to formulate its answer, which turns the LLM from a black box into a (nearly) white box.\n",
    "\n",
    "## Building a simple RAG System with llama-index\n",
    "\n",
    "For a RAG system, you need to convert your documents into plain text or Markdown, and an index to pull up relevant pieces from this corpus according to your query. There is currently gold-rush around developing converters for all kinds of documents into LLM-readable text, especially when it comes to PDFs. People try to make you to pay for this service. For PDFs, a free alternative that runs locally is **pymupdf4llm**. If your documents contain images, you can also run a multi-model LLM like llama-3.2-vision to make text descriptions for these images automatically.\n",
    "\n",
    "Once you have your documents in plain text, you can split into mouth-sized pieces (mouth-sized for your LLM and its (small) context window) and use an embedding model to compute semantic vectors for each piece. These vectors magically encode semantic meaning of text, and can be used to find pieces that are relevant to a query using cosine similiarity - that's essentially a dot-product of the vectors. It is hard to believe that this works, but it actually does. Search via embeddings is superior to keyword search, but I can also say from experience that it is not a silver bullet. The best RAG Systems combine keywords with embeddings in some way. Using a good embedding model is key. If you use a model trained for english on German text, for example, it won't perform well, or if your documents contain lots of technical language that the embedding model was not trained on.\n",
    "\n",
    "Thankfully, Ollama also offers embedding models, so you can run these locally as well. I found that mxbai-embed-large works well for both english and German text.\n",
    "\n",
    "Writing a RAG from scratch with Ollama is not too hard, but it usually pays off to use a well-designed library to do the grunt work, and then start to improve from there. I compared many libraries, and can confidently recommend **llama-index** as the best one by far. It is feature-rich and well designed: little boilerplate code for simple things, yet easy to extend. The workflow system especially is really well designed. Just their (good) documentation is annoyingly difficult to find, they try to push you to their paid cloud services (did I mention, there is a gold rush...).\n",
    "\n",
    "Below, I show you a RAG demo system, where I pull in Wikipedia pages about the seven antique world wonders, I then ask some questions about the Rhodes statue and the Hanging Gardens. As I am German, I wanted to see how well this works with German queries on German documents. That is not trivial, because both the LLM and the embedding model then have to understand German. I compare the result with and with RAG. Without RAG, the model will hallucinate details. With RAG, it follows the facts in the source documents closely. It is really impressive.\n",
    "\n",
    "To run this, you need to install a couple of Python packages:\n",
    "\n",
    "- ollama\n",
    "- llama-index\n",
    "- llama-index-llms-ollama\n",
    "- llama-index-embeddings-ollama\n",
    "- llama-index-readers-wikipedia\n",
    "- wikipedia\n",
    "- mistune\n",
    "- ipython\n",
    "\n",
    "Mistune renders Markdown to HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings, VectorStoreIndex\n",
    "from llama_index.readers.wikipedia import WikipediaReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "import textwrap\n",
    "import mistune\n",
    "from IPython.display import display_html\n",
    "\n",
    "\n",
    "def wrap(s):\n",
    "    return \"\\n\".join(textwrap.wrap(s, replace_whitespace=False))\n",
    "\n",
    "\n",
    "# logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"mxbai-embed-large\")\n",
    "\n",
    "Settings.llm = Ollama(model=\"mannix/gemma2-9b-simpo\", request_timeout=1000)\n",
    "\n",
    "Settings.text_splitter = SentenceSplitter(chunk_size=256, chunk_overlap=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from the German Wikipedia\n",
    "documents = WikipediaReader().load_data(\n",
    "    pages=[\n",
    "        \"Zeus-Statue des Phidias\",\n",
    "        \"Tempel der Artemis in Ephesos\",\n",
    "        \"Pyramiden von Gizeh\",\n",
    "        \"Pharos von Alexandria\",\n",
    "        \"Mausoleum von Halikarnassos\",\n",
    "        \"Koloss von Rhodos\",\n",
    "        \"H√§ngende G√§rten der Semiramis\",\n",
    "    ], lang_prefix=\"de\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HansDembinski\\source\\blog\\.pixi\\envs\\default\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [00:00<00:00, 128.86it/s]\n",
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 212/212 [00:12<00:00, 16.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Lots of stuff is happening here. This splits the seven pages into chunks of texts, \n",
    "# and computes an embedding vector for each chunk, in the end we have 76 chunks of text\n",
    "# that the LLM can use. We don't need to pass the embedding model or the text splitter\n",
    "# explicitly, they are pulled from the Settings object.\n",
    "index = VectorStoreIndex.from_documents(documents, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some questions for the LLM about facts regarding two of the seven wonders\n",
    "question = (\n",
    "    \"Aus welchen Materialien wurde der Koloss von Rhodos konstruiert?\",\n",
    "    \"Beschreibe die Pose des Koloss von Rhodos.\",\n",
    "    \"War der Koloss von Rhodos als nackte oder bekleidete Figur dargestellt?\",\n",
    "    \"In welcher Stadt befanden sich die H√§ngenden G√§rten?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Aus welchen Materialien wurde der Koloss von Rhodos konstruiert?</h1>\n",
       "<h2>Ohne RAG</h2>\n",
       "<p>Obwohl der genaue Materialzusammensetzung des Kolosses von Rhodos\n",
       "nicht vollst√§ndig √ºberliefert ist, k√∂nnen wir aufgrund historischer\n",
       "Beschreibungen und vergleichbarer antiker Bauwerke detailliert\n",
       "rekonstruieren, aus welchen Materialien er wahrscheinlich errichtet\n",
       "wurde:</p>\n",
       "<p><strong>1. Hauptstruktur und Sockel:</strong></p>\n",
       "<ul>\n",
       "<li><strong>Kalkstein:</strong> Dies war\n",
       "das dominierende Material f√ºr den Sockel und die Grundstruktur des\n",
       "Kolosses. Rhodos verf√ºgte √ºber lokale Kalkstein-Abbaugebiete, welche\n",
       "f√ºr massive Bauprojekte ideal waren. Kalkstein bietet Stabilit√§t,\n",
       "Widerstandsf√§higkeit gegen Witterung und erm√∂glichte die pr√§zise\n",
       "Bearbeitung gro√üer Bl√∂cke.</li>\n",
       "<li><strong>M√∂rtel:</strong> Um die Kalksteinbl√∂cke zu\n",
       "verbinden, wurde ein M√∂rtel aus Zement, Sand und Kalk verwendet.\n",
       "Dieser garantierte die dauerhafte Verbindung und Tragf√§higkeit der\n",
       "Konstruktion.</li>\n",
       "</ul>\n",
       "<p><strong>2. Skulpturale Elemente und Dekoration:</strong></p>\n",
       "<ul>\n",
       "<li><strong>Bronze:</strong>  Der Koloss selbst, insbesondere die lebensgro√üe Figur,\n",
       "war h√∂chstwahrscheinlich aus Bronze gefertigt.  Bronze war damals das\n",
       "Material der Wahl f√ºr monumentale Skulpturen aufgrund seiner\n",
       "Haltbarkeit, Formbarkeit und gl√§nzenden Oberfl√§che. Die Bronze wurde\n",
       "vermutlich durch Schmieden und Gie√üen in gro√üen Teilen hergestellt,\n",
       "welche dann zusammengef√ºgt wurden.</li>\n",
       "<li><strong>Vergoldung:</strong>  Historische\n",
       "Quellen erw√§hnen, dass der Koloss m√∂glicherweise mit einer goldenen\n",
       "Schicht bedeckt war, die seine Ausstrahlung und Pracht verst√§rkte.\n",
       "Diese Vergoldung w√§re auf Bronze-Basis aufgebracht worden und diente\n",
       "als dekorativer und symbolischer Akzent.</li>\n",
       "<li><strong>Marmor und andere\n",
       "Steine:</strong>  Detaillierte Verzierungen, Gesichter, Kleidung und\n",
       "Accessoires k√∂nnten aus Marmor, Alabaster oder anderen kostbaren\n",
       "Steinen gefertigt worden sein, um dem Koloss eine feinere und\n",
       "komplexere Optik zu verleihen.</li>\n",
       "</ul>\n",
       "<p><strong>3. Innenkonstruktion (geringf√ºgig\n",
       "bekannt):</strong></p>\n",
       "<ul>\n",
       "<li><strong>Holz und Fachwerk:</strong>  Wahrscheinlich nutzten die\n",
       "Baumeister ein System aus Holz und Fachwerk innerhalb des Kolosses, um\n",
       "die schwere Bronze-Struktur zu stabilisieren und Tragf√§higkeit zu\n",
       "gew√§hrleisten. Dieses innere Ger√ºst w√§re weniger sichtbar gewesen und\n",
       "diente der Funktionalit√§t.</li>\n",
       "</ul>\n",
       "<p>Zusammenfassend l√§sst sich sagen, dass\n",
       "der Koloss von Rhodos eine beeindruckende Kombination aus robusten\n",
       "Steinen wie Kalkstein, der formbareren Bronze f√ºr die Skulptur und\n",
       "dekorativen Elementen aus Gold, Marmor und anderen wertvollen\n",
       "Materialien darstellte. Diese Materialwahl spiegelt die technische\n",
       "Expertise und k√ºnstlerische Ambition der antiken Welt wider.</p>\n",
       "<h2>RAG</h2>\n",
       "<p>Der Koloss von Rhodos bestand aus Bronze und Eisen. F√ºr die Stabilit√§t\n",
       "wurden im Inneren der Figur Eisenger√ºste und Steine eingesetzt.</p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Beschreibe die Pose des Koloss von Rhodos.</h1>\n",
       "<h2>Ohne RAG</h2>\n",
       "<p>Der Koloss von Rhodos, obwohl nicht vollst√§ndig erhalten, l√§sst sich\n",
       "anhand von Beschreibungen aus antiken Quellen und rekonstruktiven\n",
       "Darstellungen in seiner Pose detailliert beschreiben:\n",
       "<strong>Grundhaltung:</strong> Der Koloss stand in <strong>erhobener, majest√§tischer\n",
       "Haltung</strong>, symbolisch f√ºr die St√§rke und √úberlegenheit der Insel\n",
       "Rhodos. Er war <strong>nicht sitzend oder kniend</strong>, sondern <strong>stehend auf\n",
       "zwei Beinen</strong>. Seine <strong>K√∂rperachse war leicht nach vorne geneigt</strong>,\n",
       "was Dynamik und Bewegung ins Bild brachte, obwohl er insgesamt\n",
       "statisch wirkte.</p>\n",
       "<p><strong>Arme und H√§nde:</strong>  Die Arme waren\n",
       "<strong>ausgebreitet</strong>, wobei die rechte Hand vermutlich <strong>in einer hoch\n",
       "erhobenen Geste</strong> dargestellt wurde, m√∂glicherweise einen\n",
       "Friedenskranz oder eine √§hnliche Symbolik haltend. Die linke Hand lag\n",
       "<strong>m√∂glicherweise etwas tiefer, eventuell auf einem Schwert oder einem\n",
       "anderen attributen</strong>, um Macht und Schutz zu demonstrieren.  Die\n",
       "genaue Position der H√§nde ist jedoch aufgrund fehlender √úberreste\n",
       "diskutabel.</p>\n",
       "<p><strong>Kopf und Blick:</strong> Der Kopf war <strong>hoch erhoben</strong>, der\n",
       "Blick vermutlich <strong>frontal gerichtet</strong>, symbolisch auf das Meer und\n",
       "die Reisenden gerichtet, die auf Rhodos ankamen. Dies unterstrich die\n",
       "Rolle des Kolosses als Wachmann und Garant der Sicherheit. Sein\n",
       "Gesichtsausdruck wurde wahrscheinlich als <strong>seren und weise</strong>\n",
       "interpretiert, unterstreichend die Weisheit und St√§rke der Stadt\n",
       "Rhodos.</p>\n",
       "<p><strong>Gesamteindruck:</strong> Die Pose des Kolosses war eine\n",
       "<strong>monumentale Darstellung von Macht, Schutz und friedvoller\n",
       "√úberlegenheit</strong>. Sie kombinierte Eleganz, W√ºrde und eine leicht\n",
       "dynamische Geste, um eine beeindruckende Figur zu erschaffen, die den\n",
       "Besucher von Rhodos willkommen hie√ü und gleichzeitig seine St√§rke\n",
       "symbolisierte.</p>\n",
       "<p>Obwohl der Koloss heute nur noch in Fragmenten\n",
       "existiert, l√§sst sich seine beeindruckende Pose durch die historische\n",
       "√úberlieferung und kunstgeschichtliche Analyse noch immer deutlich\n",
       "rekonstruieren und bewundern.</p>\n",
       "<h2>RAG</h2>\n",
       "<p>Obwohl keine direkte antike Darstellung existiert, wird der Koloss in\n",
       "neueren Rekonstruktionen meist als ruhig stehende Figur dargestellt,\n",
       "mit der Annahme eines gestreckten, stabilen Standbilds.  Hervorhebung\n",
       "der Pose durch einen spreizbeinigen Stand oder dynamische Bewegung\n",
       "wird in den Textquellen nicht erw√§hnt.</p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>War der Koloss von Rhodos als nackte oder bekleidete Figur dargestellt?</h1>\n",
       "<h2>Ohne RAG</h2>\n",
       "<p>Der Koloss von Rhodos, eines der sieben Weltwunder der Antike, wurde\n",
       "<strong>mit Kleidung</strong> dargestellt, obwohl Details dar√ºber, welche\n",
       "spezifischen Kleidungsst√ºcke er trug, unsicher sind.</p>\n",
       "<p><strong>Hier die\n",
       "Erkenntnisse und Schlussfolgerungen, die auf historische Hinweise und\n",
       "vergleichbare Darstellungen anderer antiker Statuen basieren:</strong></p>\n",
       "<ul>\n",
       "<li><strong>Keine eindeutigen bildlichen Aufzeichnungen:</strong>  Es existieren keine\n",
       "direkte, detaillierte Abbildungen des Kolosses, die seine Kleidung\n",
       "zweifelsfrei zeigen.</li>\n",
       "<li><strong>Traditionelle Darstellung von\n",
       "Herrscherstatuen:</strong> In der griechischen Kunst der damaligen Zeit,\n",
       "besonders bei kolossalen Statuen von G√∂ttern oder Herrschern, war es\n",
       "√ºblich, sie in <strong>kleidetem Zustand</strong> zu repr√§sentieren. Dies\n",
       "symbolisierte Macht, W√ºrde und Ansehen.  Der Koloss soll den\n",
       "Sonnengott Helios darstellen, und √§hnliche Gottheiten wurden oft mit\n",
       "Gew√§ndern oder Roben abgebildet.</li>\n",
       "<li><strong>Vergleich mit anderen Werken:</strong>\n",
       "Statuen √§hnlicher Gr√∂√üe und Funktion aus derselben Epoche, wie\n",
       "beispielsweise der Zeus-Statue in Olympia, waren ebenfalls bekleidet.\n",
       "Dies deutet auf einen vergleichbaren Stil f√ºr den Koloss hin.</li>\n",
       "<li><strong>Interpretationen und k√ºnstlerische Freiheit:</strong>  Obwohl konkrete\n",
       "Kleidungsdetails unbekannt sind,  lie√üen sich drapedierende Gew√§nder,\n",
       "eine Mantelform oder sogar milit√§rische Elemente (Rhodos war bekannt\n",
       "f√ºr seine Seefahrer und Kriegerkultur)  als plausible\n",
       "Darstellungsweisen des Kolosses interpretieren. K√ºnstler h√§tten\n",
       "basierend auf der Funktion und dem Symbolismus der Statue\n",
       "k√ºnstlerische Freiheit genossen.</li>\n",
       "</ul>\n",
       "<p><strong>Zusammenfassend l√§sst sich sagen,\n",
       "dass der wissenschaftliche Konsens darauf hindeutet, der Koloss von\n",
       "Rhodos wurde als bekleidete Figur dargestellt, obwohl die genaue Art\n",
       "der Kleidung aufgrund fehlender direkter Quellen rekonstruiert werden\n",
       "muss.</strong></p>\n",
       "<h2>RAG</h2>\n",
       "<p>Obwohl keine direkte, antike Darstellung des Kolosses existiert, l√§sst\n",
       "sich aufgrund von Schlussfolgerungen und Parallelen vermuten, dass er\n",
       "als <strong>nackter</strong> junger Mann dargestellt wurde. Der Text erw√§hnt ein\n",
       "Vorbild in M√ºnzbildern von Rhodos, die aus dem sp√§ten 5. Jahrhundert\n",
       "v. Chr. stammen und meist nackte m√§nnliche Figuren zeigen, sowie die\n",
       "g√§ngige Vorstellung von Helios als Gott, oft in der klassischen Kunst\n",
       "nackt abgebildet.  Dadurch wird eine bekleidete Darstellung\n",
       "unwahrscheinlicher.</p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>In welcher Stadt befanden sich die H√§ngenden G√§rten?</h1>\n",
       "<h2>Ohne RAG</h2>\n",
       "<p>Die H√§ngenden G√§rten, eines der Sieben Weltwunder der Antike, befanden\n",
       "sich in der <strong>Stadt Semiramis</strong> (auch bekannt als <strong>Babylon</strong>),\n",
       "gelegen im historischen Mesopotamien, dem heutigen <strong>Irak</strong>.</p>\n",
       "<p>Obwohl\n",
       "ihre genaue Lage innerhalb Babels bis heute nicht endg√ºltig gekl√§rt\n",
       "ist, vermutet man, dass sie sich in der N√§he des <strong>Palastes\n",
       "Nebukadnezars II.</strong> befanden, dem babylonischen K√∂nig, der oft mit der\n",
       "Errichtung der G√§rten in Verbindung gebracht wird. Arch√§ologische\n",
       "Funde und historische Quellen deuten auf eine Position <strong>am\n",
       "Euphratufer</strong> hin, wo Wasser f√ºr die Bew√§sserung der beeindruckenden\n",
       "Terrassenanlagen leicht verf√ºgbar gewesen w√§re.</p>\n",
       "<p>Es ist wichtig zu\n",
       "erw√§hnen, dass die Existenz und genaue Beschreibung der H√§ngenden\n",
       "G√§rten trotz zahlreicher literarischer Erw√§hnungen bis heute\n",
       "Gegenstand historischer Debatten sind.  W√§hrend arch√§ologische Beweise\n",
       "f√ºr einen komplexen Gartenbau in Babylon existieren, fehlt ein\n",
       "eindeutiger Beweis f√ºr die spezifische Architektur und Dimension der\n",
       "&quot;h h√§ngenden&quot; G√§rten, wie sie in antiken Texten beschrieben werden.\n",
       "Dennoch bleibt die Vorstellung von diesen √ºppigen, terrassierten\n",
       "G√§rten, die aus der trockenen Landschaft Babels emporragten, ein\n",
       "ikonisches Symbol f√ºr menschliche Ingenieurskunst und den Wunsch nach\n",
       "Sch√∂nheit inmitten des √ñdlands.</p>\n",
       "<h2>RAG</h2>\n",
       "<p>Der Text legt nahe, dass die H√§ngenden G√§rten in unmittelbarer N√§he\n",
       "eines Palastes und innerhalb eines Areals mit dem Namen &quot;S√ºdburg&quot;\n",
       "lagen.  Weitere Hinweise deuten auf eine Verbindung zu einem\n",
       "&quot;Nordpalast&quot; und einem Au√üenwerk hin.  Zusammenfassend l√§sst sich\n",
       "sagen, dass die H√§ngenden G√§rten innerhalb einer Stadt vermutet\n",
       "werden, die diese architektonischen Strukturen und Bezeichnungen\n",
       "aufwies.</p>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# A lot of stuff is happening here behind the scenes: a query engine is constructed\n",
    "# from the document index. The query engine computes an embedding for the query, and\n",
    "# selects 10 text pieces that are most similar to the query. It then prompts the \n",
    "# LLM with our question and provides the text pieces as context.\n",
    "engine = index.as_query_engine(similarity_top_k=10)\n",
    "\n",
    "show_sources = False\n",
    "\n",
    "# Now we ask our questions. Set show_sources=True to see which text pieces were used.\n",
    "# For reference, we compare the RAG answer (\"RAG\") with a plain LLM query (\"Ohne RAG\"). \n",
    "for q in question:\n",
    "    q2 = q + \" Antworte detailliert auf Deutsch.\"\n",
    "    \n",
    "    response = Settings.llm.complete(q2)\n",
    "    rag = engine.query(q2)\n",
    "\n",
    "    s = f\"# {q}\\n\\n## Ohne RAG\\n\\n{wrap(response.text)}\\n\\n## RAG\\n\\n{wrap(rag.response)}\"\n",
    "\n",
    "    if show_sources:\n",
    "        s += \"\\n\\n## Sources\\n\\n\"\n",
    "        for node in rag.source_nodes:\n",
    "            s += f\"### Score {node.score}\\n{wrap(node.text)}\\n\\n\"\n",
    "\n",
    "    s = mistune.html(s)\n",
    "    display_html(s, raw=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answers without the RAG are much nicer to read, but contain halluciations, while the RAG answers are dull, brief, but factually correct. The behavior of the LLM without RAG is a consequence of human preference optimization. The LLM generates answers by default that *look nice* to humans.\n",
    "\n",
    "The RAG answer is very short, because the internal prompt of llama-index asks the LLM to only use information provided by the RAG system for its answer and not use its internal knowledge. It is therefore not a bug but a feature that the answer of the LLM is so short: it tries to only make statements that are covered by the text pieces. \n",
    "\n",
    "1. Question is about the materials used to construct the Rhodes statue.\n",
    "\n",
    "The standard LLM claims that wood was used in the construction of the Rhodes statue, but there are no records in the Wikipedia at least about that. The RAG answer is factual correct, it mentions the four materials that are described in the Wikipedia article.\n",
    "\n",
    "2. Question is about the pose of the Rhodes statue.\n",
    "\n",
    "The standard LLM gives a lot of hallucinated detail. We don't know much about the pose, and the short RAG answer summarises that.\n",
    "\n",
    "3. Question is about whether the statue was clothed or naked.\n",
    "\n",
    "The standard LLM says it was clothed, probably because a lot of the antique statues were clothed, but the RAG answer is correct, the statue was naked as far as we know.\n",
    "\n",
    "4. Question is about the city in which the Hanging Gardens were supposed to be located.\n",
    "\n",
    "The standard LLM gives the correct answer in this case, Babylon. The RAG answer speaks about the location relative to the palace, but does not mention the city. This is the only case where the RAG answer is worse, although not factually incorrect. The failure in this case is related to the index, which fails to retrieve the right text piece with the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
