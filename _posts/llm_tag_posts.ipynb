{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d9cf53",
   "metadata": {},
   "source": [
    "# Tagging posts with a LLM\n",
    "\n",
    "In recent posts, explored how LLMs can be used to generate structured output from unstructured input. Since `PydanticAI` is so enjoyable to work with, I decided to use it to generate tags for my posts on this blog automatically with a LLM. For that, we let the LLM read every post and return a list of predefined tags. Since `PydanticAI` understands `Pydantic` models, we can easily contrain this list to only contain the tags we have defined. If we don't do that, models tend to invent all sorts of specific tags, but we only want broad categories.\n",
    "\n",
    "We use a local model once again, this time it is `Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS`. This model is rather large, so I cannot use a lot of context, but I managed to squeeze 24000 token into my 16 GB GPU VRAM by quantizing the KV cache with the options `--cache-type-k q8_0` and `--cache-type-v q8_0` for the `llama-server` of `llama.cpp`. `Mistral-Small-3.2-24B-Instruct-2506-IQ4_XS` works better here than `Qwen-2.5-coder-7b-instruct-Q8_0` that we had used previously. If we pass the input directly to the smaller model, it forgets its instructions and returns a summary instead of tags. It is possible to work with Qwen by splitting the task into two steps with two agents, letting the first summarize the post first and the second compute the tags, but Mistral is able to do it in one step.\n",
    "\n",
    "We save the tags as JSON in a file that Quarto (the software that generates this blog) can include to generate the categories for the posts shown on the website. If you want to know how that works, look into the git repository of this blog and search for the file `generate_metadata.py`.\n",
    "\n",
    "Since generating the tags takes a few seconds per post, we only process new posts. In order to re-tag an old post, it has to be deleted from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d509298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">llm_tag_posts.ipynb\n",
       "<span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'llm'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'programming'</span><span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "llm_tag_posts.ipynb\n",
       "\u001b[1m[\u001b[0m\u001b[32m'llm'\u001b[0m, \u001b[32m'programming'\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pydantic_ai import Agent, ModelSettings, capture_run_messages\n",
    "from pydantic_ai.providers.openai import OpenAIProvider\n",
    "from pydantic_ai.models.openai import OpenAIChatModel\n",
    "from rich import print\n",
    "import json\n",
    "import nbformat\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "model = OpenAIChatModel(\n",
    "    \"\",\n",
    "    provider=OpenAIProvider(\n",
    "        base_url=\"http://localhost:8080/v1\",\n",
    "    ),\n",
    "    settings=ModelSettings(temperature=0.5, max_tokens=1000),\n",
    ")\n",
    "\n",
    "valid_tags_raw = \"\"\"\n",
    "physics: Post is related to physics, especially particle physics.\n",
    "science: Post is about science other than physics.\n",
    "programming: Post is about programming, i.e. discussing language features or different libraries.\n",
    "high performance computing: Post is about running software efficiently and fast, typically dealing with benchmarks.\n",
    "statistics: Post is related to statistics.\n",
    "llm: Post is related to LLMs (Large Language Models) or uses LLMs, for example through agents.\n",
    "philosophy: Post touches philosophy. \n",
    "engineering: Post is about engineering.\n",
    "opinion: Post expresses opinions.\n",
    "data analysis: Post is about data analysis.\n",
    "visualization: Post is primarily about data visualization.\n",
    "graphics design: Post is about graphical design.\n",
    "parsing: Post deals with parsing input.\n",
    "bootstrap: Post is about the bootstrap method in statistics.\n",
    "uncertainty analysis: Post is about the statistical problems of error estimation, confidence interval estimation, or error propagation.\n",
    "sWeights: Posts about sWeights or COWs (custom orthogonal weight functions).\n",
    "symbolic computation: Post is about symbolic computation, e.g. with sympy.\n",
    "simulation: Post is about simulation of statistical or other processes.\n",
    "neural networks: Post is about (deep) neural networks.\n",
    "machine learning: Post is about machine learning other than with neural networks.\n",
    "prompt engineering: Post is about prompt engineering.\n",
    "web scraping: Post is about web scraping.\n",
    "environment: Post is about energy consumption and other topics that affect Earth's environment.\n",
    "\"\"\"\n",
    "\n",
    "valid_tags = {\n",
    "    v[0]: v[1] for v in (v.split(\":\") for v in valid_tags_raw.strip().split(\"\\n\"))\n",
    "}\n",
    "\n",
    "\n",
    "AllowedTags = Literal[*valid_tags]\n",
    "\n",
    "\n",
    "tag_agent = Agent(\n",
    "    model,\n",
    "    output_type=list[AllowedTags],\n",
    "    system_prompt=\"Extract broad tags that match the provided post.\",\n",
    "    instructions=f\"\"\"\n",
    "Respond with a short list of broad tags that categorize the post.\n",
    "Examples of valid tags:\n",
    "\n",
    "{\"- \".join(f\"{k}: {v}\" for (k, v) in valid_tags.items())}\n",
    "\n",
    "You must use one of these tags, you cannot invent new ones.\n",
    "\"\"\",\n",
    ")\n",
    "\n",
    "\n",
    "fn_tag_db = Path(\"../tag_db.json\")\n",
    "\n",
    "if fn_tag_db.exists():\n",
    "    with fn_tag_db.open(encoding=\"utf-8\") as f:\n",
    "        tag_db = json.load(f)\n",
    "else:\n",
    "    tag_db = {}\n",
    "\n",
    "input_files = [Path(fn) for fn in Path().rglob(\"*.*\")]\n",
    "\n",
    "for fn in input_files:\n",
    "    if fn.suffix not in (\".ipynb\", \".md\"):\n",
    "        continue\n",
    "\n",
    "    # skip files that have been processed already\n",
    "    if fn.name in tag_db:\n",
    "        continue\n",
    "\n",
    "    with open(fn, encoding=\"utf-8\") as f:\n",
    "        if fn.suffix == \".ipynb\":\n",
    "            # We clean the notebook before passing it to the LLM\n",
    "            nb = nbformat.read(f, as_version=4)\n",
    "            nb.metadata = {}\n",
    "            for cell in nb.cells:\n",
    "                if cell.cell_type == \"code\":\n",
    "                    cell.outputs = []\n",
    "                    cell.execution_count = None\n",
    "                    cell.metadata = {}\n",
    "            doc = nbformat.writes(nb)\n",
    "        elif fn.suffix == \".md\":\n",
    "            doc = f.read()\n",
    "\n",
    "    tag_input = f\"{fn!s}:\\n\\n{doc}\"\n",
    "    with capture_run_messages() as messages:\n",
    "        try:\n",
    "            result = await tag_agent.run(tag_input)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            # If there is an error (typically a schema validation error),\n",
    "            # print the messages for debugging.\n",
    "            print(messages)\n",
    "            break\n",
    "    print(fn.name, result.output)\n",
    "    tag_db[fn.name] = result.output\n",
    "\n",
    "    # save after every change, in case something breaks\n",
    "    with fn_tag_db.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(tag_db, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394ce23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
