{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unbiasedness of EML fit for a mixture model with fixed component pdfs\n",
    "\n",
    "The log-likelihood for one observed Poisson-distributed count is (without constants)\n",
    "$$\n",
    "\\ell_i(\\lambda) := \\ln \\mathcal{L}_i(\\lambda) = -\\lambda + k_i \\ln \\lambda.\n",
    "$$\n",
    "\n",
    "It is easy to show that the ML estimate, $\\hat\\lambda = k_i$, is unbiased,\n",
    "\n",
    "$$\n",
    "E[\\hat \\lambda] = E[k_i] = \\lambda.\n",
    "$$\n",
    "\n",
    "This can be generalised: if the estimator is a linear function of the observations, the estimator is unbiased.\n",
    "\n",
    "We now consider a two component model, where the number density is $\\rho(x) = S f_s(x) + B f_b(x)$.\n",
    "\n",
    "For one bin $i$, the expectation is $\\lambda_i = S s_i + B b_i$ with constants $s_i = \\int_{a_i}^{b_i} f_s(x) dx$ and $b_i = \\int_{a_i}^{b_i} f_b(x) dx$.\n",
    "\n",
    "We construct the estimator by solving the score functions.\n",
    "\n",
    "$$\n",
    "\\ell_i(S, B) = -(S s_i + B b_i) + k_i \\ln(S s_i + B b_i)\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial \\ell_i}{\\partial S} = -s_i + k_i \\frac{s_i}{S s_i + B b_i} \\overset!= 0 \\quad\\text{and}\\quad\n",
    "\\frac{\\partial \\ell_i}{\\partial B} = -b_i + k_i \\frac{b_i}{S s_i + B b_i} \\overset!= 0\n",
    "$$\n",
    "\n",
    "Both equations give the same condition\n",
    "$$\n",
    "\\Rightarrow \\hat S s_i + \\hat B b_i = k_i, \n",
    "$$\n",
    "which allows infinitely many solutions for one bin i, but for two bins or more, it has a unique solution. We give it explicitly for two bins.\n",
    "$$\n",
    "\\hat S s_1 + \\hat B b_1 = k_1 \\\\\n",
    "\\hat S s_2 + \\hat B b_2 = k_2\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat S = \\frac{k_1 - \\frac{b_2}{b_1} k_2}{s_1 - s_2 \\frac{b_1}{b_2}}\n",
    "$$\n",
    "and analog for $\\hat B$. The estimates are linear functions of the observations $k_1$ and $k_2$ and thus unbiased.\n",
    "\n",
    "Unbiasedness for an unbinned EML fit then follows, since the unbinned EML fit is a limiting case of the binned EML fit as the bin width goes to zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
